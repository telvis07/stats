---
title: "Stats 101"
author: "technicalelvis.com"
output: 
  html_document:
    toc: true
    theme: cosmo
    keep_md: true

fig_width: 5 
fig_height: 3 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(knitr)
library(ggplot2)
library(gridExtra)
library(mvtnorm)
```


## Introduction to Probability Distributions

This document lists common probability distributions described in

* [Introduction to Probability by Joseph Blizstein and Jessica Hwang](https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view). 

* [Little Inference Book by Brian Caffo](https://leanpub.com/LittleInferenceBook)


For each distribution, we show both the _probability density_ and _cumulative distribution_ plots. 

**Probability density** plots show the probability that a random variable $X$ equals some value $x$; expressed using the notation: $P(X=x)$. 

**Cumulative distribution** plots show the probablity that a random variable $X$ _is less than or equal to_ some value $x$; expressed using the notation $P(X<=x)$.

The cumulative distribution answers questions like: 

* "probability a person's height is __greater than__ 4 feet"
* "probability a cookie has __less than__ 10 chocolate chips"
* "probability a shopping cart has __greater than__ 3 items". 

The __emphasized__ phrases indicate we want the total population with a value in a certain range. 
This contrasts with probability density, that answers questions like:

* "probability a person's height __exactly equals__ 4 feet"
* "probability a cookie has __exactly__ 10 chocolate chips"
* "probability a shopping cart has __exactly__ 3 items".

The __emphasized__ phrases indicate we want the slice of the population that exactly equals a certain value. 

### Setting lower.tail parameter in R

The `R` distribution functions (eg. pbinom, ppois, pnorm, etc.) have a `lower.tail` option to get the range above or below a threshold. 

* Set `lower.tail = FALSE` to get the population percentage __above__ a threshold.
* Set `lower.tail = TRUE` to get the population percentage __below__ a threshold.


### Distribution vs Density

In general, the term `distribution` is used to describe statistics on discrete variables (eg. item counts, customer counts, etc). The `density` applies to 
continuous variables that can take any value from $-inf$ to $+inf$ (eg. heights, weights, etc. )


## Binomial
Suppose that _n_ independent coin flips (ie. Bernoulli trials) are performed, each with the same success probability p. Let
_X_ be the number of HEADS (ie. successes). The distribution of X is called a _Binomial distribution_. 

### Binomial Density
The plot shows the probability of getting *exactly* _x_ HEADS in 10 biased coin flips where the probability of HEADS is 20%.

```{r dbinom, echo=F, fig.width=6, fig.height=4, fig.align = "center"}
chap3.1 <- function(){
  # Prob x=1:5 where X ~ Bin(5, 0.2)
  x <- seq(1, 10, 1)
  y <- dbinom(x, 10, 0.2)
  
  df <- data.frame(x=x, y=y)
  g <- ggplot(df, aes(x=x, y=y)) +
    geom_point() +
    scale_x_discrete(name="x", limits=x) +
    geom_linerange(aes(x=x, ymax=y, ymin=0)) +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(title="Binomial Density")
  g
}
chap3.1()
```


### Binomial Cumulative distribution

The plot shows the probability of getting greater than _x_ HEADS in 10 coin flips where the probability of HEADS is 20%.

```{r pbinom, echo=F, fig.width=6, fig.height=4, fig.align = "center"}
chap3.2 <- function(){
  # Prob x<1:10 where X ~ Bin(5, 0.2)
  x <- 1:10
  y <- pbinom(x, 10, 0.2, lower.tail = F)

  df <- data.frame(x=x, y=y)
  g <- ggplot(df, aes(x=x, y=y)) +
    geom_line() +
    scale_x_discrete(name="x", limits=x) +
    ylab("y") +
    theme(plot.title = element_text(hjust = 0.5)) +
    labs(title="Binomial Cumulative Density")
  g

}
chap3.2()
```



## Poisson

The Poisson distribution is often used in situations where we are counting the number of successes in a particular region or interval of time and there are a large number of trials. Each trial has a small probability of success. The parameter lambda ($\lambda$) is interpreted as the rate of occurrence of these rare events.

$\lambda$ could be 20 (emails per hour), 10 (chips per cookie), and 2 (earthquakes per year). The poisson paradigm says that in applications similar to the ones above, we can approximate the distribution of the number of events that occur by a Poisson Distribution.

### Poisson Density
Consider an example where the number of people that show up at a bus stop is Poisson with a mean of 2.5 per hour. If watching the bus stop for 1 hours, what is the 
probability that exactly $x$ people show up for the whole time?

```{r dpois, echo=F,  fig.width=6, fig.height=4, fig.align = "center"}
chap3.poisson.story.dpois <- function(num_success=3, rate=2.5, interval=1) {
  # Notice the multiplication by four in the function argument. Since lambda is specified as 
  # events per hour we have to multiply by four to consider the number of events that occur in 4 hours.
  lambda <- rate * interval
  x <- 1:10
  df <- data.frame(x=x)
  
  df['y'] <- dpois(x, lambda)
  p <- ggplot(df, aes(x=x, y=y)) +
       geom_point() +
       geom_linerange(aes(x=x, ymax=y, ymin=0)) +
       scale_x_discrete(name="x", limits=x) +
       ylab("y") +
       theme(plot.title = element_text(hjust = 0.5)) +
       labs(title="Poisson Density")
  
  p
}

chap3.poisson.story.dpois()
```


### Poisson Cumulative Distribution
For the previous example, what is the probability that more than $x$ people show up for the whole time?

```{r ppois, echo=F,  fig.width=6, fig.height=4, fig.align = "center"}
chap3.poisson.story.ppois <- function(num_success=3, rate=2.5, interval=1) {
  # Notice the multiplication by four in the function argument. Since lambda is specified as 
  # events per hour we have to multiply by four to consider the number of events that occur in 4 hours.
  lambda <- rate * interval
  x <- 0:10
  df <- data.frame(x=x)
  
  df['y'] <- ppois(x, lambda, lower.tail = F)
  p <- ggplot(df, aes(x=x, y=y)) +
    geom_line() +
    scale_x_discrete(name="x", limits=x) +
    labs(y="y") + 
    labs(x="x: number of events") +
    labs(title="Poisson Cumulative Distribution")
    
  
  p
}

chap3.poisson.story.ppois()
```

### Simulating a Poisson Process containing 1 event type
The following example illustrates how to simulate arrival times within a specified interval $(0,L]$. 

1. Generate the number of arrivals $N(L)$ from $Pois(\lambda*L)$. 
2. Conditional on $N(L) = n$, generate arrival times from ordered `Unif(O,L)`. 

The following plot simulates arrivals from a Poisson process with rate 10 events/interval after observing for 5 intervals.

```{r poisson1, echo=F, fig.width=6, fig.height=4, fig.align="center"}
pois.1 <- function() {

  # 5 intervals
  L <- 5
  
  # 10 events / interval
  lambda <- 10
  
  # randomly generate number of events using pois(lambda*L)
  n <- rpois(1, lambda*L)
  
  # generate random "times" from 0 through L for "n" events, then sort them
  t <- sort(runif(n, 0, L))
  
  # visualize L events. 
  # x=times each n event occurs
  # y=event index
  df <- data.frame(x=t, y=1:n)
  g <- ggplot(df, aes(x=x, y=y) ) +
    geom_point() +     
    scale_y_continuous(name= "N(t): Number of successes at t") +
    scale_x_continuous(name= "Time t (intevals)")
  
  g
}

pois.1()
```

### Simulating a Poisson Process containing 2 event types

Now, let's build on the previous example to model a Poisson process that contains 2 event types.

1. Generate the number of arrivals $N(L)$ from $Pois(\lambda*L)$. 
2. Conditional on $N(L) = n$, generate arrival times from ordered `Unif(O,L)`. 
3. For each arrival, we flip a coin with probability `Bern(0.3)` of Heads; these coin tosses are labeled as `type-1`; the rest are labeled as `type-2`. 
  
The resulting vectors of arrival times t1 and t2 are realizations of 2 independent Poisson processes.

```{r poisson2, echo=F, fig.width=6, fig.height=4, fig.align="center"}
pois.2 <- function() {
  
  # 5 intervals
  L <- 5
  
  # 10 events/interval
  lambda <- 10
  
  # randomly generate number of events using pois(lambda*L)
  n <- rpois(1, lambda*L)
  
  # generate random "times" from 0 through L for "n" events, then sort them
  t <- sort(runif(n, 0, L))
  
  # type-1 event probabilty: p=0.3
  p <- 0.3
  y <- rbinom(n, 1, p)
  
  # assign "event times" as type-1 or type-2 
  t1 <- t[y==1]
  t2 <- t[y==0]
  
  # as before, we can plot the number of arrivals in each Poisson process: N_1(t) and N_2(t).
  
  # visualize the plot for type #1 events
  df1 <- data.frame(x=t1, y=1:length(t1))
  p1 <- ggplot(df1, aes(x=x, y=y) ) +
    geom_point(shape=16, color="blue")  +
    theme(
          axis.ticks.y=element_blank()) +
  ylab("type1: count") +
    theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
  
  # visualize the plot for type #2 events
  df2 <- data.frame(x=t2, y=1:length(t2))
  p2 <- ggplot(df2, aes(x=x, y=y) ) +
    geom_point(shape=17, color="orange")  +
    theme(
          axis.ticks.y=element_blank()) +
    ylab("type2: count") +
      theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
  
  # visualize the plot for both event types
  df1$event_type <- "type-1"
  df2$event_type <- "type-2"
  df3 <- rbind(df1, df2)
  df3$y <- 1:length(t)
  
  p3 <- ggplot(df3, aes(x=x, y=y, color=event_type) ) +
    geom_point()  +
    theme(
          axis.ticks.y=element_blank()) +
    ylab("combined") +
    xlab("Time t (intervals)") +
    scale_shape_manual(values=c(16, 17))+
    scale_color_manual(values=c('blue','orange')) +
    theme(legend.position="none")
  
  grid.arrange(p1, p2, p3, nrow=3)
}

pois.2()
```

## Gaussian

The Normal distribution is a famous continuous distribution with a bell-shaped PDF. It is extremely widely used in statistics because of a theorem,
the [central limit theorem](https://github.com/telvis07/StatsInf_PeerAssessment1/blob/master/project.md), which says under very weak assumptions, the sum of a large number of i.i.d. random variables has an approximately Normal 
distribution - regardless of the distribution of the underlying random variable.

The simplest normal distribution is the _standard normal_, which is centered at 0 and has a variance of 1.

### Gaussian Density

The following plot shows the probability that `Normal(0,1)` *exactly equals* any value between -3 and +3.

 
```{r dnorm, echo=F, fig.width=6, fig.height=4, fig.align = "center"}
 do_dnorm <- function() {
 x <- seq(-3, 3, 0.01)
 y <- dnorm(x)
 datums <- data.frame(x=x, y=y)
 g <- ggplot(datums, aes(x, y)) + geom_line() + 
 theme(plot.title = element_text(hjust = 0.5)) +
 scale_x_discrete(name="x", limits=seq(-3, 3, 1)) +
 labs(title="Normal Density")
 
 g
}
 
do_dnorm()
```


### Gaussian Cumulative Distribution

The following plot shows the probability that `Normal(0,1)` is *greater than* to any value between -3 and +3.

 
```{r pnorm, echo=F, fig.width=6, fig.height=4, fig.align="center"}
do_pnorm <- function() {
   # x <- seq(0, 1, 0.001)
   x <- seq(-3, 3, 0.01)
   y <- pnorm(x, lower.tail = F)
   df <- data.frame(x=x, y=y)
   ggplot(df, aes(x=x, y=y)) + geom_line() +
     theme(plot.title = element_text(hjust = 0.5)) +
     scale_x_discrete(name="x", limits=seq(-3, 3, 1)) +
     labs(title="Normal Cumulative Distribution")
}

do_pnorm()
```

### Click Fraud Rate Simulation

Lets' consider a [click fraud detection event analysis](https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection) where 1 out of 100 ad clicks is fraudulent (ie. fraud rate is 1%).

If we had a very large dataset, we could run `2000` experiments where we calculate the fraud rate of small samples. According to the [central limit theorem](https://github.com/telvis07/StatsInf_PeerAssessment1/blob/master/project.md), we can take the average fraud rate across all the experiments and be close to the actual fraud rate.


```{r fraudrate, echo=F, fig.width=6, fig.height=4, fig.align="center"}
fraudrate.sim <- function(p=0.1, nosim=2000, n=500) {
  # p
  q <- 1 - p
  
  # calc the means
  mean.pop <- p
  
  # calc the variance
  var.pop <- p*q
  
  # Generate simulated data
  rbin_samples <- matrix(rbinom(n*nosim, 1, prob = p), nosim, n) 
  
  # Calc sample means for the 1000 simulations
  rbin_samples_means <- matrix(apply(rbin_samples, 1, mean))
  
  rbin_samples_means
}

avgs_across_coin_flips <- function(nosim=100) {
  df <- data.frame(
    x = c(fraudrate.sim(p=0.1, nosim=nosim, n=10),
          fraudrate.sim(p=0.1, nosim=nosim, n=100),
          fraudrate.sim(p=0.1, nosim=nosim, n=200),
          fraudrate.sim(p=0.1, nosim=nosim, n=500)),
    size = factor(rep(c(10, 100, 200, 500), rep(nosim, 4)))
  )
  
  g <- ggplot(df, aes(x=x, fill=size)) +
       geom_histogram(alpha=0.20, bins=20, color="black") +
       facet_grid(. ~ size)    
  g
}

avgs_across_coin_flips()
```

The plot above shows a histogram of the fraud rates for each sample. We try sizes of 10, 100, 200 and 500 samples. Notice that as the sample size grows, the distribution gets Gaussian looking (like a bell curve) and increasinsly centered at 0.01 (1%).

### Multivariate Normal

A multivariate (MVN) normal distribution generalizes the Normal distribution into higher dimension. The parameters of a multi-variate normal are:

1. The _mean vector_ ($\mu_1, \mu_2, ... \mu_n$)
2. The _covariance matrix_, which is the _k x k_ matrix of covariances between components, arranged so that the
row _i_, column _j_ entry is Cov($X_i, X_j$).

The following plot shows random variables from 2 bivariate normals: `BLUE` and `ORANGE`. The `BLUE` distribution has parameters
$N((1,0)^T, I)$. The `ORANGE` distribution has parameters $N((0,1)^T, I)$. **I** represents a 2x2 identity matrix - where the diagonals are `1` and other values are `0`.

```{r mvnplot, echo=F, fig.width=6, fig.height=4, fig.align = "center"}
do_mvtnorm <- function(mu1, mu2, s1=1, s2=1, rho_0=0, N=10) {
  # dvmnorm can be used for calculating the joing PDF and rmvnorm can be used for 
  # generating random vectors.
  meanvector <- c(mu1,mu2)

  # The covariance matrix is (assuming sd(Z)=s1=1, sd(W)=s2=1)
  # [(1, rho), (rho, 1)]
  # because:
  # Cov(Z, Z) = Var(Z) = 1 (this is the upper left entry)
  # Cov(W, W) = Var(W) = 1 (this is the lower right entry)
  # Cov(Z, W) = Corr(Z,W) * sd(Z) * sd(W) = rho 
  # 
  # NOTE (if rho_0=1, then covmatrix equals IDENTITY matrix(c(1, 0, 0, 1), 2))
  covmatrix = matrix(c(s1^2, s1*s2*rho_0, s1*s2*rho_0, s2^2), nrow=2, ncol=2)

  # now r  is N x 2 matrix, with each row a BVN random vector. 
  r <- rmvnorm(n=N, mean=meanvector, sigma=covmatrix)

  r
}

generate_mvn_plot <- function(N=100) {
  # N((1,0)^T, I) labeled class 'BLUE'
  bvn1 <- do_mvtnorm(mu1=1, mu2=0, s1=1, s2=1, rho_0=0, N=N)
  df1 <- data.frame(bvn1)
  colnames(df1) <- c("x1", "x2")
  df1["label"] = "BLUE"

  # N((0,1)T, I) labeled class 'ORANGE'
  bvn2 <- do_mvtnorm(mu1=0, mu2=1, s1=1, s2=1, rho_0=0,  N=N)
  df2 <- data.frame(bvn2)
  colnames(df2) <- c("x1", "x2")
  df2["label"] = "ORANGE"

  # stack the dataframes
  df <- rbind(df1, df2)

  # Plot BLUE vs ORANGE
  g <- ggplot(df, aes(x=x1, y=x2, color=label)) +
    geom_point() +
    scale_color_manual(breaks = c("BLUE", "ORANGE"),
                       values=c("blue", "orange"))
  g
}

generate_mvn_plot()
```


